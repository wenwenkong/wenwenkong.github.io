---
layout: post
title: GenAI with LLMs (3) Instruction fine-tuning
date: 2025-06-21
description: Course summary of Generative AI with Large Language Models. Part Three.
tags: llm
#categories: course-summary
giscus_comments: false
related_posts: false
thumbnail: assets/img/posts/genai_llm_3/1_finetuning_highlevel.png
toc:
  sidebar: left
---

This post covers instruction fine-tuning from the **Generative AI With LLMs** course offered by DeepLearning.AI.

## LLM fine-tuning at a high level

<div class="row mt-3">
     <div class="col-sm mt-3 mt-md-0">
         {% include figure.liquid loading="eager" path="assets/img/posts/genai_llm_3/1_finetuning_highlevel.png" class="img-fluid rounded z-depth-1" %}
     </div>
</div>
<div class="caption">
     <b>Figure 1.</b> LLM fine-tuning at a high level. Source: course lecture.
</div>

#### Why we need LLM fine-tuning

Recall In-context learning: zero-shot, one-shot, and few-shot inference. See [this note](https://wenwenkong.com/blog/2025/gen-ai-llm-1/#prompt-engineering) for more details.

We need LLM fine-tuning because in-context learning has several drawbacks:

- For smaller models, the in-context learning doesn't always work, even when 5 or 6 examples are included.
- Any examples you include in your prompt take up valuable space in the context window, reducing the amount of room you have to include other useful information.

#### What is LLM fine-tuning

Fine-tuning is a <b>supervised learning</b> process, where you use a dataset of labeled examples to update the weights of the LLM.

<b>Instruction fine-tuning</b> trains the model using examples demonstrating how it should respond to a specific instruction. The labeled examples are <b>prompt-completion pairs</b>; the fine-tuning process extends the training of the model to improve its ability to generate good completions for a specific task.

Each example in the prompt-completion pairs datasets begins with <b>instructions</b>. For example, if you want to fine-tune your model to improve its summarization ability, you'd build up a dataset of examples that begin with instruction "summarize the following text" or a similar phrase; if you are improving the model's translation skills, your examples would include instructions like "translate this sentence". These examples allow the model to learn to generate responses following given responses.

<div class="row mt-3">
     <div class="col-sm mt-3 mt-md-0">
         {% include figure.liquid loading="eager" path="assets/img/posts/genai_llm_3/2_finetuning_highlevel.png" class="img-fluid rounded z-depth-1" %}
     </div>
</div>
<div class="caption">
     <b>Figure 2.</b> Using prompts to fine-tune LLMs with instruction. Source: course lecture.
</div>

<b>Instruction fine-tuning</b>, where all of the model's weights are updated, is known as <b>full fine-tuning</b>. The process results in a new version of the model with updated weights. Note that just like pre-training, full fine-tuning requires enough memory and compute budget to store and process all the gradients, optimizers, and other components that are being updated during training.

## LLM fine-tuning process

First step of instruction fine-tuning is to prepare your training data. There is much publicly available data that has been used to train earlier generations of LLMs, but not all of them are formatted as instructions. Developers have assembled [prompt template libraries](https://github.com/bigscience-workshop/promptsource/tree/main/promptsource/templates) that can be used to take existing dataset. For example,turn the large dataset of Amazon product review into instruction prompt datasets for fine-tuning (see <b>Figure 3</b>, [source](https://github.com/bigscience-workshop/promptsource/blob/main/promptsource/templates/amazon_polarity/templates.yaml)). In each case, you pass in the original review, here called `review_body`, to the template, where it gets inserted into the text that starts with an instruction. The result is a prompt that now contains both an instruction and the example from the dataset.

<div class="row mt-3">
     <div class="col-sm mt-3 mt-md-0">
         {% include figure.liquid loading="eager" path="assets/img/posts/genai_llm_3/3_prompt_instruction_template.png" class="img-fluid rounded z-depth-1" %}
     </div>
</div>
<div class="caption">
     <b>Figure 3.</b> Sample prompt instruction templates. Source: course lecture.
</div>

Once the instruction dataset is ready, we divide the dataset into `training`, `validation`, `test` split.

During fine-tuning, we select prompts from the training set and pass them to the LLM which then generates completions. Next, we compare LLM completion with the response specified in the training data.

<div class="row mt-3">
     <div class="col-sm mt-3 mt-md-0">
         {% include figure.liquid loading="eager" path="assets/img/posts/genai_llm_3/4_finetuning_process.png" class="img-fluid rounded z-depth-1" %}
     </div>
</div>
<div class="caption">
     <b>Figure 4.</b> LLM fine-tuning process. Source: course lecture.
</div>

Recall that the output of an LLM is a probability distribution across tokens. So we can compare the distribution of the completion and that of the training label, and use the standard [<b>cross entropy function</b>](https://en.wikipedia.org/wiki/Cross-entropy) to calculate <b>loss</b> between the two token distributions. Then use the calculated loss to update model weights in standard <b>backpropagation</b>. Do this for many batches of prompt completion pairs and over several epochs, update the weights so that the model's performance on the task improves. Then get the <b>validation_accuracy</b> using the holdout validation set, and get the <b>test_accuracy</b> once you apply the model to the test set.

## Single task vs. Multi-task

### Single task fine-tuning

Good results can be achieved with relatively few examples (often 500-1000 examples) for single task fine-tuning. However, fine-tuning on a single task can lead to <b>catastrophic forgetting</b>, i.e. fine-tuning significantly improves performance of the model on a specific task but degrades performance on other tasks.

How to avoid catastrophic forgetting?

- First of all, you might not have to if you only care about the task you fine-tuned for
- Fine-tune on multiple tasks at the same time
- Consider Parameter Efficient Fine-tuning (PEFT)

### Multi-task fine-tuning

<div class="row mt-3">
     <div class="col-sm mt-3 mt-md-0">
         {% include figure.liquid loading="eager" path="assets/img/posts/genai_llm_3/5_multitask_finetuning.png" class="img-fluid rounded z-depth-1" %}
     </div>
</div>
<div class="caption">
     <b>Figure 5.</b> Multi-task fine-tuning. Source: course lecture.
</div>

Over many epochs of training, the calculated losses across examples are used to update the weights of the model, resulting in an instruction fine-tuned model that learned how to be good at many different tasks simultaneously.

Drawback to multi-task fine-tuning: requires a lot of data. However, it can be very worthwhile and worth the effort to assemble this data. The resulting models are often very capable and suitable for use in situations where good performance at many tasks is desirable.

## Instruction fine-tuning with FLAN

FLAN == <b>F</b>ine-tuned <b>LA</b>nguage <b>N</b>et. FLAN models refer to a specific set of instructions used to perform instruction fine-tuning. FLAN-T5 (fine-tuned version of pre-trained T5 model) is a great, general purpose, instruct model.

<div class="row mt-3">
     <div class="col-sm mt-3 mt-md-0">
         {% include figure.liquid loading="eager" path="assets/img/posts/genai_llm_3/6_FLAN.png" class="img-fluid rounded z-depth-1" %}
     </div>
</div>
<div class="caption">
     <b>Figure 6.</b> Datasets and task categories employed in FLAN fine-tuning, <a href="https://arxiv.org/abs/2210.11416" target="_blank">source</a>.
</div>

## LLM evaluation

#### Metrics: ROUGE

<b>ROUGE</b> is used for text summarization and text generation tasks. Figures in below demonstrate usage of <b>ROUGE-1</b>, <b>ROUGE-2</b>, <b>ROUGE-L</b>, and <b>ROUGE clipping</b>.

<b>ROUGE-1</b> measures the overlap of <b>unigrams (single words)</b> between the generated and reference texts.

<div class="row mt-3">
     <div class="col-sm mt-3 mt-md-0">
         {% include figure.liquid loading="eager" path="assets/img/posts/genai_llm_3/7_ROUGE_1.png" class="img-fluid rounded z-depth-1" %}
     </div>
</div>
<div class="caption">
     <b>Figure 7.</b> ROUGE-1. Source: course lecture.
</div>

<b>ROUGE-2</b> measures the overlap of <b>bigrams (two-word sequences)</b> between the generated and reference texts.

<div class="row mt-3">
     <div class="col-sm mt-3 mt-md-0">
         {% include figure.liquid loading="eager" path="assets/img/posts/genai_llm_3/8_ROUGE_2.png" class="img-fluid rounded z-depth-1" %}
     </div>
</div>
<div class="caption"> 
     <b>Figure 8.</b> ROUGE-2. Source: course lecture.
</div>

<b>ROUGE-L</b> measures the <b>longest common subsequences (LCS)</b> between the generated and reference texts, capturing setence-level fluency and structure.

<div class="row mt-3">
     <div class="col-sm mt-3 mt-md-0">
         {% include figure.liquid loading="eager" path="assets/img/posts/genai_llm_3/9_ROUGE_L.png" class="img-fluid rounded z-depth-1" %}
     </div>
</div>
<div class="caption"> 
     <b>Figure 9.</b> ROUGE-L. Source: course lecture.
</div>

<b>ROUGE clipping</b> limits the count of overlapping n-grams to the maximum number that appears in the reference, preventing the generated text from getting extra credit for repeated words.

<div class="row mt-3">
     <div class="col-sm mt-3 mt-md-0">
         {% include figure.liquid loading="eager" path="assets/img/posts/genai_llm_3/10_ROUGE_clipping.png" class="img-fluid rounded z-depth-1" %}
     </div>
</div>
<div class="caption"> 
     <b>Figure 10.</b> ROUGE clipping. Source: course lecture.
</div>

#### Metrics: BLEU SCORE

<b>BLEU SCORE</b> is used for text translation. Below is the core BLEU formula, from [Papineni et al. 2002](https://aclanthology.org/P02-1040.pdf).

$$
\text{BLEU} = \text{BP} \cdot \exp\left( \sum_{n=1}^{N} w_n \log p_n \right)
$$

- **BLEU**: The final BLEU score (0 to 1), evaluating how closely the generated text matches the reference.
- **BP**: _Brevity Penalty_ — penalizes candidates that are shorter than the reference.
- **$$\exp$$**: Exponential function, used to combine the log-precisions into a geometric mean.
- **$$w_n$$**: Weight for each n-gram order (e.g., 0.25 when using up to 4-grams).
- **$$p_n$$**: _Modified precision_ for n-grams of size $$n$$, with clipping to avoid over-counting.
- **$$N$$**: Maximum n-gram order (usually 4 in practice).

#### Benchmarks

Selecting an evaluating dataset is vital to an accurate evaluation of model performance. Example evaluation benchmarks include [GLUE](https://gluebenchmark.com/), [SuperGLUE](https://super.gluebenchmark.com/), [MMLU (Massive Multitask Language Understanding)](https://paperswithcode.com/dataset/mmlu), [HELM](https://crfm.stanford.edu/helm/), [Big-bench](https://github.com/google/BIG-bench), etc.

## References

- [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf)
  - Scaling fine-tuning with a focus on task, model size and chain-of-thought data.
- [Introducing FLAN: More generalizable Language Models with Instruction Fine-Tuning](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html)
  - This blog (and article) explores instruction fine-tuning, which aims to make language models better at performing NLP tasks with zero-shot inference.
- [HELM - Holistic Evaluation of Language Models](https://crfm.stanford.edu/helm/latest/)
  - HELM is a living benchmark to evaluate Language Models more transparently.
- [General Language Understanding Evaluation (GLUE) benchmark](https://openreview.net/pdf?id=rJ4km2R5t7)
  - This paper introduces GLUE, a benchmark for evaluating models on diverse natural language understanding (NLU) tasks and emphasizing the importance of improved general NLU systems.
- [SuperGLUE](https://super.gluebenchmark.com/)
  - This paper introduces SuperGLUE, a benchmark designed to evaluate the performance of various NLP models on a range of challenging language understanding tasks.
- [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013.pdf)
  - This paper introduces and evaluates four different measures (ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S) in the ROUGE summarization evaluation package, which assess the quality of summaries by comparing them to ideal human-generated summaries.
- [BLEU: a Method for Automatic Evaluation of Machine Translation](https://aclanthology.org/P02-1040.pdf)
- [Measuring Massive Multitask Language Understanding (MMLU)](https://arxiv.org/pdf/2009.03300.pdf)
  - This paper presents a new test to measure multitask accuracy in text models, highlighting the need for substantial improvements in achieving expert-level accuracy and addressing lopsided performance and low accuracy on socially important subjects.
- [BigBench-Hard - Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models](https://arxiv.org/pdf/2206.04615.pdf)
  - The paper introduces BIG-bench, a benchmark for evaluating language models on challenging tasks, providing insights on scale, calibration, and social bias.
