<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://wenwenkong.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://wenwenkong.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-24T21:34:06+00:00</updated><id>https://wenwenkong.github.io/feed.xml</id><title type="html">blank</title><subtitle># add description here if you want </subtitle><entry><title type="html">Predicting U.S. Soybean Yield with Climate Data</title><link href="https://wenwenkong.github.io/blog/2025/us-soybean-prediction/" rel="alternate" type="text/html" title="Predicting U.S. Soybean Yield with Climate Data"/><published>2025-03-11T00:00:00+00:00</published><updated>2025-03-11T00:00:00+00:00</updated><id>https://wenwenkong.github.io/blog/2025/us-soybean-prediction</id><content type="html" xml:base="https://wenwenkong.github.io/blog/2025/us-soybean-prediction/"><![CDATA[<h2 id="summary">Summary</h2> <p>This post walks through my end-to-end machine learning project on predicting U.S. soybean annual yield based on local climate conditions. We cover all key steps of a machine learning workflow, including ideation, feature selection, cross validation, baseline modeling, hyperparameter tuning, and model deployment. We employed annual soybean yield data and monthly climate variables during the historical period from 1981 to 2016. The model was trained on annual soybean yield data and monthly climate variables from 1981 to 2015, with 2016 used as the test set. We found that XGBRegressor performed the best among the regression algorithms (including linear and tree-based models) tested in this project. Our model achieved an \(R^2\) of ~0.9, demonstrating a strong ability to predict U.S. soybean annual yield based on climate conditions. Please refer to <a href="https://github.com/wenwenkong/data-science-portfolio/tree/main/US_soybean_yield">this repo</a> for the projectâ€™s code.</p> <hr/> <h2 id="introduction">Introduction</h2> <p>Soybean is a vital source of protein for both human and animal nutrition, but its yield is highly sensitive to weather and climate variability. High temperatures and low soil moisture, particularly during the summer reproductive period, can significantly reduce soybean yields (Hamed et al., 2021). Early-season excessive precipitation can also negatively impact soybean yields by restricting root development, causing nutrient leaching, and increasing disease susceptibility (Ortiz-Bobea et al., 2019).</p> <p>This project is motivated by two questions:</p> <ol> <li>Can we build a machine learning model that predicts local annual soybean yield in the United States based on local climate conditions?</li> <li>Which climatic factors contribute most to the soybean yield prediction?</li> </ol> <p><strong>Figure 1</strong> (<a href="https://www.codecademy.com/article/deep-learning-workflow">source</a>) illustrates the life cycle of a typical end-to-end machine learning project, which consists of four main components: data preparation, feature selection, model training, and deployment. We closely followed this framework throughout the project.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/us_soybean_yield/fig_1_ML_Workflow-480.webp 480w,/assets/img/posts/us_soybean_yield/fig_1_ML_Workflow-800.webp 800w,/assets/img/posts/us_soybean_yield/fig_1_ML_Workflow-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/us_soybean_yield/fig_1_ML_Workflow.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 1.</b> Life cycle of an end-to-end machine learning project. </div> <p>The reminder of the post is structured as follows. We first introduce the dataset and conduct an exploratory data analysis to examine the temporal and spatial patterns of the historical U.S. soybean yield. We then describe the feature selection process that constructed the training dataset. The model training section covers key aspects of model development. Finally, we discuss the deployment process and conclude with an overview of caveats and potential directions for future work.</p> <hr/> <h2 id="data">Data</h2> <p>We focused on two datasets: one for soybean yield and the other for climate variables. Historical annual soybean yield was provided by the Global Dataset of Historical Yields (GDHY) (lizumi and Sakai, 2020), where crop yield is defined as production per unit harvested area (\(t\) \(ha^{-1}\)). For climate variables, we used monthly model outputs from the North American Land Data Assimilation System (NLDAS) (Mitchell et al. 2004). The NLDAS dataset provides near-surface climate variables, including surface energy fluxes, surface water flux and storage, soil moisture, temperature, and land surface parameters. Both GDHY and NLDAS are land-based datasets, meaning that values over the ocean appear as NaNs. We focus on the period 1981-2016, covering the spatial domain \(235.25^{\circ} - 292.75^{\circ}E, 25.25^{\circ} - 52.75^{\circ}N\), which encompasses the primary continental regions in the U.S.</p> <hr/> <h2 id="exploratory-data-analysis-eda">Exploratory Data Analysis (EDA)</h2> <p>To better understand U.S. soybean production, we conducted an exploratory data analysis (<strong>Figures 2-5</strong>). The majority of soybean production is concentrated in the eastern and midwestern states, while the Great Plains region has relatively lower production density (<strong>Figure 2</strong>). To complement this spatial view, <strong>Figure 3</strong> presents a boxplot of the interquartile range (IQR) across all soybean production grid points for each year, highlighting spatial variations in yield Both local soybean yield and its spatial spread exhibit year-to-year variability (<strong>Figures 2-3</strong>).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/us_soybean_yield/fig_2_GDHY_Soybean_4yrs-480.webp 480w,/assets/img/posts/us_soybean_yield/fig_2_GDHY_Soybean_4yrs-800.webp 800w,/assets/img/posts/us_soybean_yield/fig_2_GDHY_Soybean_4yrs-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/us_soybean_yield/fig_2_GDHY_Soybean_4yrs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 2.</b> Soybean annual yield (unit: \(t\) \(ha^{-1}\)) in the U.S. for four years. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/us_soybean_yield/fig_3_GDHY_Box-480.webp 480w,/assets/img/posts/us_soybean_yield/fig_3_GDHY_Box-800.webp 800w,/assets/img/posts/us_soybean_yield/fig_3_GDHY_Box-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/us_soybean_yield/fig_3_GDHY_Box.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 3.</b> Boxplot of the U.S. soybean annual yield for each year. </div> <p>Statistical analysis reveals that U.S. soybean annual yield follows a bimodal distribution (<strong>Figure 4</strong>), suggesting that not all soybean-producing regions behave uniformly. We hypothesize that this bimodal pattern arises from differences between high-production areas (Midwestern and Eastern states) and lower-production areas (Great Plains) The left mode of the distribution likely corresponds to the Great Plains, while the right mode represents more densely cultivated regions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/us_soybean_yield/fig_4_GDHY_PDF-480.webp 480w,/assets/img/posts/us_soybean_yield/fig_4_GDHY_PDF-800.webp 800w,/assets/img/posts/us_soybean_yield/fig_4_GDHY_PDF-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/us_soybean_yield/fig_4_GDHY_PDF.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 4.</b> Probability Density Function of the U.S. soybean annual yield from 1981 to 2016. </div> <p>Both climatology and long-term trends display a west-east dipole pattern, further the Great Plains apart from other areas(<strong>Figure 5</strong>). Climatologically, soybean yield is lower in the Great Plains compared to other regions. The long-term trend, however, highlights the Great Plains as a hotspot area that experienced notable increase in soybean yield from 1981 to 2016. The overall increasing trend in the U.S. Soybean yield aligns with what we observed in <strong>Figure 3</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/us_soybean_yield/fig_5_GDHY_climatology_trend-480.webp 480w,/assets/img/posts/us_soybean_yield/fig_5_GDHY_climatology_trend-800.webp 800w,/assets/img/posts/us_soybean_yield/fig_5_GDHY_climatology_trend-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/us_soybean_yield/fig_5_GDHY_climatology_trend.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 5.</b> (Left) Climatology and (Right) trend of the U.S. soybean annual yield. </div> <p>In this project, we train a unified model for all soybean production regions across the U.S. This approach assumes that the causal relationship between local climate conditions and soybean yield is consistent across all soybean production regions. We acknowledge that this is an oversimplification and we discuss its limitations in the <code class="language-plaintext highlighter-rouge">Caveats and Future Work</code> section.</p> <p>We merged the GDHY and NLDAS datasets by matching the <code class="language-plaintext highlighter-rouge">lat</code>, <code class="language-plaintext highlighter-rouge">lon</code>, and <code class="language-plaintext highlighter-rouge">year</code> features (see <strong>Figure 6</strong> for a screenshot of the merged dataset; see section 4.1 in <a href="https://github.com/wenwenkong/data-science-portfolio/blob/main/US_soybean_yield/1.0_Data_ingestion.ipynb">this notebook</a>). The resulting DataFrame contains over 600 climate features from NLDAS. We will perform feature selection in the next section.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/us_soybean_yield/fig_6_GDHY_NLDAS_merged_head-480.webp 480w,/assets/img/posts/us_soybean_yield/fig_6_GDHY_NLDAS_merged_head-800.webp 800w,/assets/img/posts/us_soybean_yield/fig_6_GDHY_NLDAS_merged_head-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/us_soybean_yield/fig_6_GDHY_NLDAS_merged_head.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 6.</b> Screenshot of the merged DataFrame of the GDHY (i.e. soybean yield) and NLDAS (i.e. climate variables) datasets. </div> <hr/> <h2 id="feature-selection">Feature selection</h2> <p>Feature selection requires critical thinking and iterative analysis, making it the most tedious component of this project. Our goal is to narrow down the climate features to a subset that most likely affect soybean yield. This section outlines our thought process for feature selection. Detailed calculations and reasoning can be found in <a href="https://github.com/wenwenkong/data-science-portfolio/blob/main/US_soybean_yield/2.0_EDA_feature_engineering_selection_OutputsCleared.ipynb">this notebook</a> and <a href="https://github.com/wenwenkong/data-science-portfolio/blob/main/US_soybean_yield/3.0_feature_selection_model_training.ipynb">this notebook</a>.</p> <p><strong>Key considerations for feature selection</strong>:</p> <ul> <li>Feature selection should only be performed using the training dataset to prevent bias in feature and model choice from the test set.</li> <li>Feature selection does not always guarantee improved predictions.</li> <li>Linear and non-linear feature selection methods may yield conflicting results. When this happens, we rely on domain knowledge and our understanding of the problem to make decisions.</li> </ul> <p><strong>Feature selection process</strong></p> <p>We break the feature selection process into two phases:</p> <ul> <li>Phase 1: Initial feature filtering based on correlation analysis and domain knowledge.</li> <li>Phase 2: Further refinement using scikit-learnâ€™s feature selection methods. We used features derived from Phase 1 to build baseline models, and used the refined features from Phase 2 for final model selection (see the <code class="language-plaintext highlighter-rouge">Model Development</code> section).</li> </ul> <p><strong>Phase 1: Initial Filtering</strong></p> <p>We followed these key principles in Phase 1:</p> <ol> <li>Temporal consideration: <ul> <li>Most U.S. soybeans are planted in May and early June and harvested in late September and October (Source: <a href="https://www.ers.usda.gov/topics/crops/soybeans-oil-crops/oil-crops-sector-at-a-glance/">USDA</a>). Thus, we only consider climate features during months before October, removing November and December variables from the current year.</li> <li>Besides current-year climate, we explored whether past-year climate conditions matter. In particular, we explored past yearâ€™s annual total rainfall, past winterâ€™s snowfall, rainfall, and soil moisture.</li> </ul> </li> <li>Correlation analysis: <ul> <li>We examined correlation between annual yield and climate features to identify important features. We kept variables with strong correlation with yield, and discarded the rest.</li> <li>Since some relationships between climate and yield are nonlinear, we kept certain features despite weak linear correlations if they were scientifically relevant.</li> </ul> </li> <li>Redundant feature removal: <ul> <li>We aggregated (e.g., seasonal averages) climate variables that are highly auto-correlated across months to avoid redundancy.</li> <li>Some climate variables exhibit strong cross-correlations, andi we only kept the most relevant ones.</li> <li>We excluded vegetation and land cover features (such as leaf area index) that directly reflect the yield data during the soybean growing and harvest season.</li> </ul> </li> <li>Feature engineering: We created new features when necessary. For example, we derived an evaporative fraction using surface energy fluxes and used it instead of latent and sensible heat fluxes to simplify the feature set.</li> </ol> <p><strong>Phase 2: scikit-learn based feature selection methods</strong></p> <p>In Phase 2, we refined the feature set using various scikit-learn methods: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html">VarianceThreshold</a>, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html">f_regression</a>, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html">mutual_info_regression</a>, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html">Lasso</a>, and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html">SelectFromModel</a>. Although typically fewer methods are needed in practical applications, here we experimented with multiple approaches for the sake of practice. More details can be found in <a href="https://github.com/wenwenkong/data-science-portfolio/blob/429f607bb8148e9792c85c149e3f381056ad8346/US_soybean_yield/3.0_feature_selection_model_training.ipynb">this notebook</a>.</p> <hr/> <h2 id="model-development">Model Development</h2> <h3 id="cross-validation">Cross-validation</h3> <p><a href="https://machinelearningmastery.com/k-fold-cross-validation/">Cross-validation</a> helps reduce overfitting in machine learning models. While scikit-learnâ€™s train_test_split` method allows for random splitting of data, it is not suitable for our problem due to the sequential nature of the dataset. Instead, we implemented an expanding window backtesting procedure for cross-validation (<strong>Figure 7</strong>). Using this approach, we created a 5-fold split in the training set, ensuring that the test size remained constant across splits for comparability in their performance statistics:</p> <ul> <li>Split 1: train on 1980-2010, test on 2011</li> <li>Split 2: train on 1980-2011, test on 2012</li> <li>Split 3: train on 1980-2012, test on 2013</li> <li>Split 4: train on 1980-2013, test on 2014</li> <li>Split 5: train on 1980-2014, test on 2015</li> </ul> <p>This method ensures that each model is trained on an expanding historical dataset, reflecting real-world forecasting conditions where future predictions are based only on past information.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/us_soybean_yield/fig_7_Expanding-window-cross-validation-480.webp 480w,/assets/img/posts/us_soybean_yield/fig_7_Expanding-window-cross-validation-800.webp 800w,/assets/img/posts/us_soybean_yield/fig_7_Expanding-window-cross-validation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/us_soybean_yield/fig_7_Expanding-window-cross-validation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 7.</b> Illustration of the expanding window backtesting concept. Adopted from Uber blog <a href="https://www.uber.com/blog/omphalos/" target="_blank">here</a>. </div> <h3 id="model-selection">Model selection</h3> <p>We first explored both linear models (OLS, Ridge, Lasso) and tree-based models (Decision Tree, Random Forest, XGBoost) to build baseline models. We used both \(R^2\) and RMSE to evaluate model performance. Not surprisingly, tree-based models outperformed linear models, with XGBoost achieving the best performance among the three tree-based models (<strong>Figure 8</strong>).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/us_soybean_yield/fig_8_Model_selection-480.webp 480w,/assets/img/posts/us_soybean_yield/fig_8_Model_selection-800.webp 800w,/assets/img/posts/us_soybean_yield/fig_8_Model_selection-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/us_soybean_yield/fig_8_Model_selection.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 8.</b> Box plot of \(R^{2}\) values across cross-validation splits from (left) linear regression models and (right) tree-based regression models. </div> <h3 id="hyperparameter-tuning">Hyperparameter tuning</h3> <p>We focused on tuning the following XGBoost hyperparameters to control overfitting:</p> <ul> <li><code class="language-plaintext highlighter-rouge">n_estimators</code>: Number of rounds for boosting.</li> <li><code class="language-plaintext highlighter-rouge">learning_rate</code>: Step size shrinkage used in update to prevent overfitting.</li> <li><code class="language-plaintext highlighter-rouge">colsample_bytree</code>: Subsample ratio of columns when constructing each tree</li> <li><code class="language-plaintext highlighter-rouge">subsample</code>: Subsample ratio of the training instances.</li> <li><code class="language-plaintext highlighter-rouge">gamma</code>: Minimum loss reduction required to split a leaf node of the tree; higher values make the algorithm more conservative.</li> <li><code class="language-plaintext highlighter-rouge">min_child_weight</code>: Minimum sum of instance weights needed in a child; higher values make the algorithm more conservative. -<code class="language-plaintext highlighter-rouge">max_depth</code>: Maximum depth of a tree; higher values increase model complexity and risk of overfitting.</li> <li><code class="language-plaintext highlighter-rouge">reg_alpha</code>: L1 regularization term on weights; higher values make the model more conservative.</li> </ul> <p>Grid Search, Random Search, and Bayesian Optimization are commonly used hyperparameter tuning methods. We tested both Random Search and Bayesian Optimization (via <a href="https://optuna.readthedocs.io/en/stable/">Optuna</a>). We selected the RandomizedSearchCV-tuned parameters as they produced a slightly higher \(R^2\) value.</p> <h3 id="feature-importance">Feature importance</h3> <p>We used XGBoostâ€™s built-in function to evaluate feature importance (<strong>Figure 9</strong>). Both longitude and year emerge as key features, reflecting the spatial-temporal nature of soybean yield prediction. Longitude serves as a proxy for geographic differences in soybean production, as observed in the EDA, while year likely captures long-term trends affecting yield. Several variables related to soil moisture content also stand out, including spring and summer soil moisture and summertime evaporative fraction. Springtime leaf area index also contributes to yield prediction, likely because it reflects pre-season vegetation health, which could correlate with soil fertility and growing conditions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/us_soybean_yield/fig_9_XGBoost_feature_importance-480.webp 480w,/assets/img/posts/us_soybean_yield/fig_9_XGBoost_feature_importance-800.webp 800w,/assets/img/posts/us_soybean_yield/fig_9_XGBoost_feature_importance-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/us_soybean_yield/fig_9_XGBoost_feature_importance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 9.</b> Feature importance chart evaluated based on XGBoost built-in function. </div> <hr/> <h2 id="inference-and-deployment">Inference and Deployment</h2> <p>How well does the model perform in 2016 soybean annual yield? <strong>Figure 10</strong> compares the true versus predicted values, suggesting that the model does a decent job in capturing both the spatial pattern and local magnitude of soybean yield. However, the model overall tends to underestimate annual yield c, while overestimating yield in some localized areas. We did not investigate the cause of this underestimation, which could be an area for future analysis. We deployed the trained XGBoost model on AWS EC2, and built a <a href="http://www.ussoybean-demo.com/">web interface</a> using Flask, basic css and javascript.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/us_soybean_yield/fig_10_XGBoost_prediction-480.webp 480w,/assets/img/posts/us_soybean_yield/fig_10_XGBoost_prediction-800.webp 800w,/assets/img/posts/us_soybean_yield/fig_10_XGBoost_prediction-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/us_soybean_yield/fig_10_XGBoost_prediction.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 10.</b> (Top) Actual value and (Middle) predicted value of the U.S. soybean annual yield for 2016. The bottom plot shows fractional error between the actual and predicted values. </div> <hr/> <h2 id="caveats-and-future-work">Caveats and Future Work</h2> <p>In this section, we discuss several limitations of the current approach and potential future improvements.</p> <p><strong>Limited predictors</strong></p> <p>Unlike some other crops (e.g. corn) that are strongly influenced by soil type, drainage, population, row width, tillage and other physical and human factors, soybean yield is more dependent on the natural environment (Source: <a href="https://www.agweb.com/news/crops/soybeans/stack-odds-soybeans-spring">Stack the odds for soybeans this spring</a>). Still, additional factors such as genotype, soil type, and seeding experiments could improve our modelâ€™s predictability.</p> <p>In this project, we focused solely on the predictive power of local monthly climate due to the lack of suitable datasets for other predictors. This limitation may have constrained our modelâ€™s predictability. Even for the climate data, we relied on a single source (NLDAS). Future work could involve testing alternative datasets that provide both atmospheric and land-based variables to validate or refine our findings.</p> <p><strong>Coarse timescale of crop yield</strong></p> <p>The crop yields data used in this project only provide annual yields, thus we lack information on the timing of soybean planting and seasonal growth cycle at each location and year. Several studies (Colet et al., 2023; Vann and Stokes, 2024) suggest that planting timing affects yield. Future work incorporating sub-annual yield estimates or phenological data could help capture these temporal variations.</p> <p><strong>Localized climate perspective</strong></p> <p>This work takes a localized perspective of climate impacts.hat is, we only considered impacts of local climate on local yield. However, large-scale climate patterns can also affect U.S. soybean yield through teleconnections. For example, El NiÃ±o-Southern Oscillation (ENSO) events can impact U.S. growing conditions by synchronizing climate risks across major agricultural regions (Anderson et al., 2018). Including remote climate indices as features could improve our understanding of remote climate influences on soybean yield.</p> <p><strong>Spatial variation</strong></p> <p>As shown in the EDA, U.S. soybean yield varies across the Great Plains, Midwest, and Eastern states. We hypothesize that different sub-regions are governed by different climate factors. Even a common set of climate features is relevant across all regions, their importance may vary by location. Future work could explore training separate models for different sub-regions to improve predictions for local areas.</p> <p><strong>LSTM as an alternative model architecture</strong></p> <p>Long Short-Term Memory (LSTM) networks are well-suited for time-series forecasting tasks. Given the temporal nature of our soybean yield prediction problem, incorporating LSTM in future work could provide valuable insights. Several studies have demonstrated the effectiveness of LSTMs in crop yield prediction (Sun et al, 2019; Bhimavarapu et al., 2023). However, while LSTMs excel at capturing sequential dependencies, they require larger datasets and are more computationally intensive than tree-based models like XGBoost.</p> <hr/> <h2 id="references">References</h2> <p><strong>Journal Articles:</strong></p> <ul> <li>Anderson, W., Seager, R., Baethgen, W., &amp; Cane, M. (2018). Trans-Pacific ENSO teleconnections pose a correlated risk to agriculture. Agricultural and Forest Meteorology. <a href="https://doi.org/10.1016/j.agrformet.2018.07.023">https://doi.org/10.1016/j.agrformet.2018.07.023</a></li> <li>Bhimavarapu, U., Battineni, G., &amp; Chintalapudi, N. (2023). Improved optimization algorithm in LSTM to predict crop yield. Computers, 12(1), 10. <a href="https://doi.org/10.3390/computers12010010">https://doi.org/10.3390/computers12010010</a></li> <li>Colet, F., Lindsey, A. J., &amp; Lindsey, L. E. (2023). Soybean planting date and seeding rate effect on grain yield and profitability. Agronomy Journal. <a href="https://doi.org/10.1002/agj2.21434">https://doi.org/10.1002/agj2.21434</a></li> <li>Hamed, M. K., Vogel, M. M., Patricola, C. M., &amp; Seneviratne, S. I. (2021). Impacts of compound hotâ€“dry extremes on US soybean yields. Earth System Dynamics, 12(4), 1371â€“1386. <a href="https://doi.org/10.5194/esd-12-1371-2021">https://doi.org/10.5194/esd-12-1371-2021</a></li> <li>Iizumi, T., &amp; Sakai, T. (2020). The global dataset of historical yields for major crops 1981â€“2016. Scientific Data, 7(1), 97. <a href="https://doi.org/10.1038/s41597-020-0433-7">https://doi.org/10.1038/s41597-020-0433-7</a></li> <li>Mitchell, K. E., Lohmann, D., Houser, P. R., Wood, E. F., Schaake, J. C., Robock, A., â€¦ &amp; Cosgrove, B. A. (2004). The multi-institution North American Land Data Assimilation System (NLDAS): Utilizing multiple GCIP products and partners in a continental distributed hydrological modeling system. Journal of Geophysical Research: Atmospheres, 109(D7), D07S90. <a href="https://doi.org/10.1029/2003JD003823">https://doi.org/10.1029/2003JD003823</a></li> <li>Ortiz-Bobea, A., Wang, H., Carrillo, C. M., &amp; Ault, T. R. (2019). Unpacking the climatic drivers of U.S. agricultural yields. Environmental Research Letters, 14(6), 064003. <a href="https://doi.org/10.1088/1748-9326/ab1e75">https://doi.org/10.1088/1748-9326/ab1e75</a></li> <li>Sun, J., Di, L., Sun, Z., Shen, Y., &amp; Lai, Z. (2019). County-level soybean yield prediction using deep CNN-LSTM model. Sensors, 19(20), 4363. <a href="https://doi.org/10.3390/s19204363">https://doi.org/10.3390/s19204363</a></li> </ul> <p><strong>Online Tutorials &amp; Blog Posts:</strong></p> <ul> <li>Vann, R., &amp; Stokes, D. J. (2024, January). How does soybean planting date impact plant height and soybean yield? NC State Extension. <a href="https://soybeans.ces.ncsu.edu/2024/01/how-does-soybean-planting-date-impact-plant-height-and-soybean-yield/">https://soybeans.ces.ncsu.edu/2024/01/how-does-soybean-planting-date-impact-plant-height-and-soybean-yield/</a></li> <li>AgWeb. (2024, February). Stack the odds for soybeans this spring. <a href="https://www.agweb.com/news/crops/soybeans/stack-odds-soybeans-spring">https://www.agweb.com/news/crops/soybeans/stack-odds-soybeans-spring</a></li> <li>XGBoost. (n.d.). XGBoost hyperparameter tuning guide. XGBoost Documentation. <a href="https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html">https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html</a></li> <li>XGBoost. (n.d.). XGBoost parameters. XGBoost Documentation. <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">https://xgboost.readthedocs.io/en/stable/parameter.html</a></li> <li>Amazon Web Services. (n.d.). Tune an XGBoost model in Amazon SageMaker. AWS Documentation. <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html</a></li> <li>Uber. (n.d.). Omphalos: Uberâ€™s point of interest engine for better user experiences. Uber Blog. <a href="https://www.uber.com/blog/omphalos/">https://www.uber.com/blog/omphalos/</a></li> <li>Towards Data Science. (2020, April). Grid search vs. random search vs. Bayesian optimization. <a href="https://towardsdatascience.com/grid-search-vs-random-search-vs-bayesian-optimization-2e68f57c3c46">https://towardsdatascience.com/grid-search-vs-random-search-vs-bayesian-optimization-2e68f57c3c46</a></li> <li>Practical Data Science. (n.d.). How to tune an XGBRegressor model with Optuna. <a href="https://practicaldatascience.co.uk/machine-learning/how-to-tune-an-xgbregressor-model-with-optuna">https://practicaldatascience.co.uk/machine-learning/how-to-tune-an-xgbregressor-model-with-optuna</a></li> </ul> <p><strong>GitHub Repository:</strong></p> <ul> <li>Kong, W. (2022). U.S. soybean yield prediction [GitHub repository]. <a href="https://github.com/wenwenkong/data-science-portfolio/tree/main/US_soybean_yield">https://github.com/wenwenkong/data-science-portfolio/tree/main/US_soybean_yield</a></li> </ul>]]></content><author><name>Wenwen Kong</name></author><category term="machine-learning"/><category term="climate"/><summary type="html"><![CDATA[An end-to-end machine learning project on U.S. soybean yield prediction]]></summary></entry><entry><title type="html">Statistical Learning Notes Outline</title><link href="https://wenwenkong.github.io/blog/2023/sl-notes-outline/" rel="alternate" type="text/html" title="Statistical Learning Notes Outline"/><published>2023-07-27T00:00:00+00:00</published><updated>2023-07-27T00:00:00+00:00</updated><id>https://wenwenkong.github.io/blog/2023/sl-notes-outline</id><content type="html" xml:base="https://wenwenkong.github.io/blog/2023/sl-notes-outline/"><![CDATA[<p>This note outlies topics about statistical learning that I want to update. Each note serves as a summary of my learnings, including concepts, R/Python methods, and use cases. Though ISLR2 will be the main reference, readings and learnings from other resources (such as blog posts or papers) will be cited when necessary.</p> <hr/> <h3 id="topics">Topics</h3> <ol> <li> <p>Fundamentals</p> </li> <li> <p>Linear Regression</p> </li> <li> <p>Lasso, Ridge, and ElasticNet</p> </li> <li> <p>Logistic Regression</p> </li> <li> <p>Tree-based models (Decision Trees, Random Forests, Bagging, Boosting)</p> </li> <li> <p>XGBoost</p> </li> <li> <p>Regression and Classification Metrics</p> </li> <li> <p>Support Vector Machine</p> </li> <li> <p>Unsupervised Learning</p> </li> </ol> <p>â€¦ â€¦</p>]]></content><author><name></name></author><category term="stats-learning"/><category term="statistics"/><summary type="html"><![CDATA[Outline of notes about statistical learning.]]></summary></entry><entry><title type="html">Statistics and Climate Journal Club</title><link href="https://wenwenkong.github.io/blog/2022/stats-climate-journalclub/" rel="alternate" type="text/html" title="Statistics and Climate Journal Club"/><published>2022-09-11T00:00:00+00:00</published><updated>2022-09-11T00:00:00+00:00</updated><id>https://wenwenkong.github.io/blog/2022/stats-climate-journalclub</id><content type="html" xml:base="https://wenwenkong.github.io/blog/2022/stats-climate-journalclub/"><![CDATA[<p>Statistics and Climate Journal Club at UCLA was founded by <a href="https://samjbaugh.github.io">Sam Baugh</a> and myself in October 2020, thanks to the suggestion and help from <a href="https://karenamckinnon.github.io/">Professor Karen McKinnon</a>.</p> <p>Since then, we have been running the journal club at a bi-weekly cadence and hosted the discussion primarily on the virtual sphere due to the COVID. These discussions are made possible by contributions and participations of many colleagues at UCLA, especially those who have served as discussion leads. We also hosted a few seminar talks, delivered by speakers who generously shared their research with us remotely.</p> <h4 id="journal-club-archive-in-reversed-chronological-order">Journal club archive in reversed chronological order</h4> <hr/> <h5 id="winter-2022">Winter 2022</h5> <p>03/02/2022 Paper discussion</p> <ul> <li>Joint discussion on <a href="https://link.springer.com/article/10.1007/s10584-021-03226-6">Shepherd, 2021, Bringing physical reasoning into statistical practice in climate-change science</a></li> </ul> <p>02/16/2022 Invited seminar</p> <ul> <li>Title: Atmospheric Physics-Guided Machine Learning: Towards Physically-Consistent, Data-Driven, and Interpretable Models of Convection</li> <li>Speaker: Tom Beucler</li> <li>Readings: <ul> <li><a href="https://journals.ametsoc.org/view/journals/atsc/77/12/jas-d-20-0082.1.xml">Brenowitz, Beucler, Pritchard, and Bretherton, 2020, Interpreting and stabilizing machine-learning parameterizations of convection</a></li> <li><a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.126.098302">Beucler et al., 2021, Climate-Invariant Machine Learning</a></li> <li><a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.126.098302">Beucler et al., 2021, Enforcing analytic constraints in neural networks emulating physical systems</a></li> </ul> </li> </ul> <p>02/02/2022 Paper discussion</p> <ul> <li><a href="https://www.science.org/doi/10.1126/science.1227079">Sugihara et al., 2012, Detecting Causality in Complex Ecosystems</a> led by Mengxi Wu</li> </ul> <hr/> <h5 id="fall-2021">Fall 2021</h5> <p>12/03/2021 Paper discussion</p> <ul> <li><a href="https://wcd.copernicus.org/articles/2/971/2021/wcd-2-971-2021.pdf">Terray, 2021, A dynamical adjustment perspective on extreme event attribution</a> led by Suqin Duan</li> </ul> <p>11/19/2021 Invited seminar</p> <ul> <li>Title: Strengthened Causal Connections Between the MJO and the North Atlantic With Climate Warming</li> <li>Speaker: Savini Samarasinghe</li> <li>Readings: <ul> <li><a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2020GL091168">Samarasinghe et al., 2021, Strengthened Causal Connections Between the MJO and the North Atlantic With Climate Warming</a></li> </ul> </li> </ul> <p>11/05/2021 Invited seminar</p> <ul> <li>Topic: A method for Detection and Attribution of Regional Precipitation Change Using Granger Causality</li> <li>Speaker: Mark Risser</li> <li>Related readings: <ul> <li><a href="https://doi.org/10.1038/nclimate2976">Sarojini et al., 2016, Detection and attribution of human influence on regional precipitation</a></li> <li><a href="https://doi.org/10.1175/BAMS-D-13-00212.1">Hegerl et al., 2015, Challenges in quantifying changes in the global water cycle</a></li> <li><a href="https://doi.org/10.1175/JCLI-D-17-0672.1">Knutson and Zeng, 2018, Model assessment of observed precipitation trends over land regions: Detectable human influences and possible low bias in model trends</a></li> <li><a href="https://doi.org/10.1073/pnas.1921628117">Kirchmeier-Young and Zhang, 2020, Human influence has intensified extreme precipitation in North America</a></li> <li><a href="https://link.springer.com/article/10.1007/s00382-022-06321-1">Risser et al., 2023, A framework for detection and attribution of regional precipitation change: Application to the United States historical record</a></li> </ul> </li> </ul> <p>10/22/2021 Paper discussion</p> <ul> <li><a href="https://www.nature.com/articles/s41558-018-0379-3">Mori et al., 2019, A reconciled estimate of the influence of Arctic sea-ice loss on recent Eurasian cooling</a> led by Weiming Ma</li> </ul> <p>10/08/2021 Paper discussion</p> <ul> <li><a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2019JD031024">Barnes et al., 2019, Tropospheric and Stratospheric Causal Pathways Between the MJO and NAO</a> led by Fiaz Ahmed</li> </ul> <hr/> <h5 id="spring-2021">Spring 2021</h5> <p>06/11/2021 Paper discussion</p> <ul> <li><a href="https://www.nature.com/articles/nature14550">Horton et al., 2015, Contribution of changes in atmospheric circulation patterns to extreme temperature trends</a> led by Yizhou Zhuang</li> </ul> <p>04/16/2021 Paper discussion</p> <ul> <li><a href="https://www.frontiersin.org/journals/earth-science/articles/10.3389/feart.2019.00355/full">Wrzesien and Pavelsky (2020): Projected changes to extreme runoff and precipitation events from a downscaled simulation over the western United States</a> led by Stefan Rahimi</li> </ul> <p>04/02/2021 &amp; 05/14/2021 Paper discussion</p> <ul> <li><a href="https://journals.ametsoc.org/view/journals/clim/32/17/jcli-d-18-0882.1.xml">Sippel et al. 2019 Uncovering the Forced Climate Response from a Single Ensemble Member Using Statistical Learning</a> led by Gavin D. Madakumbura</li> </ul> <hr/> <h5 id="winter-2020">Winter 2020</h5> <p>03/17/2021 Paper discussion</p> <ul> <li><a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2017GL076327">Wills et al. 2018, Disentangling Global Warming, Multidecadal Variability, and El NiÃ±o in Pacific Temperatures</a> led by Wenwen Kong</li> </ul> <p>03/03/2021 Paper discussion</p> <ul> <li><a href="https://link.springer.com/article/10.1007/s10584-013-1021-z">Hewitson et al. 2014, Interrogating empirical-statistical downscaling</a> led by Naomi Goldenson</li> </ul> <p>02/17/2021 Paper discussion</p> <ul> <li><a href="https://onlinelibrary.wiley.com/doi/10.1002/env.1147">Katzfuss et al. 2017, Bayesian hierarchical spatio-temporal smoothing for very large datasets</a> led by Sam Baugh</li> </ul> <p>02/03/2021 Paper discussion</p> <ul> <li><a href="https://www.nature.com/articles/s41558-020-0731-2">Deser et al. 2020, Insights from Earth system model initial-condition large ensembles and future prospects</a> led by Jesse Norris</li> </ul> <p>01/20/2021 - <a href="https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2020JC016459">Raphael et al. 2020, An assessment of the temporal variability in the annual cycle of daily Antarctic sea ice in the NCAR Community Earth System Model, version 2: a comparison of the historical runs with observations</a> led by Thomas Maierhofer</p> <hr/> <h5 id="fall-2020">Fall 2020</h5> <p>12/09/2020 Paper discussion</p> <ul> <li><a href="https://www.nature.com/articles/s41558-019-0666-7">Sippel et al. 2020, Climate change now detectable from any single day of weather at global scale</a> led by Wenwen Kong</li> </ul> <p>11/25/2020 Paper discussion</p> <ul> <li><a href="https://link.springer.com/article/10.1007/s00382-016-3079-6">Ribes et al. 2017, A new statistical approach to climate change detection and attribution</a> led by Gavin D. Madakumbura</li> </ul> <p>10/28/2020 Paper discussion</p> <ul> <li><a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2018.1451335">Risser et al. 2018, Spatially dependent multiple testing under model misspecification, with application to detection of anthropogenic influence on extreme climate events</a> led by Sam Baugh</li> </ul> <p>10/14/2020 Kick-off meeting</p> <ul> <li>Introductions, discussion topics and formats, suggestions, etc.</li> </ul>]]></content><author><name></name></author><category term="climate-stats"/><category term="statistics"/><category term="climate"/><summary type="html"><![CDATA[Archived topics discussed in the Statistics and Climate Journal Club at UCLA.]]></summary></entry></feed>