{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3028ab9d",
   "metadata": {},
   "source": [
    "## Notebook content\n",
    "- Equation (1): Patch embedding + class token + positional embeddings → `z₀`\n",
    "- Equations (2)–(3): Transformer encoder blocks (MSA + MLP) with pre-LN and residuals\n",
    "- Equation (4): Final LayerNorm on the class token and a linear classification head\n",
    "- Printed shapes at each stage to match the math\n",
    "\n",
    "**<center>Equation components breakdown</center>**\n",
    "\n",
    "| Equation    | Component                             | This notebook                                |\n",
    "| ----------- | --------------------------------------|--------------------------------------------- | \n",
    "| **(1)**     | Patch embedding (image → patches → embeddings) | Unfold + Linear, class token, positional embeddings |\n",
    "| **(2)**     | Multi-Head Self-Attention (MSA)                | MultiHeadSelfAttention              | \n",
    "| **(3)**     | Feed-forward network (MLP)                     | MLP                                 |       \n",
    "| **(2)+(3)** | Transformer encoder layer                      | ViTEncoderLayer                                   \n",
    "| **(4)**     | Final LayerNorm and classification head        | LayerNorm + Linear                  | \n",
    "\n",
    "\n",
    "**<center>Dimension symbol meanings and typical values in ViT-Base</center>**\n",
    "\n",
    "| Symbol              | Meaning                                          | Typical Value (ViT-Base)              |\n",
    "| ------------------- | ------------------------------------------------ | ------------------------------------- |\n",
    "| **B**               | Batch size — number of images processed together | 1, 8, 32, …                           |\n",
    "| **H, W**            | Image height and width (input)                   | 224 × 224                             |\n",
    "| **P**               | Patch size (each side length in pixels)          | 16                                    |\n",
    "| **C**               | Number of input channels                         | 3 (RGB)                               |\n",
    "| **N**               | Number of patches per image                      | (H / P) × (W / P) = 14 × 14 = **196** |\n",
    "| **D**               | Embedding dimension (hidden size)                | 768                                   |\n",
    "| **h** or **H_attn** | Number of attention heads                        | 12                                    |\n",
    "| **Dₕ**              | Dimension *per head*                             | D / h = 768 / 12 = **64**             |\n",
    "\n",
    "---\n",
    "\n",
    "## Mini-implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ccc676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa6edf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a2cd2a",
   "metadata": {},
   "source": [
    "### Eq. (2) — Multi-Head Self-Attention (MSA) sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f8ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention (Eq. 2)\"\"\"\n",
    "    def __init__(self, dim, num_heads=12, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):  # x: [B, N, D]\n",
    "        B, N, D = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, H, N, Dh]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale          # [B, H, N, N]\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        out = attn @ v                                         # [B, H, N, Dh]\n",
    "        out = out.transpose(1, 2).reshape(B, N, D)             # [B, N, D]\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69196bf",
   "metadata": {},
   "source": [
    "### Eq. (3) — MLP sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0286e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Two-layer feedforward block (Eq. 3 part).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, mlp_ratio=4.0, act_layer=nn.GELU, drop=0.0):\n",
    "        super().__init__()\n",
    "        hidden = int(dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(dim, hidden)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden, dim)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e43a3",
   "metadata": {},
   "source": [
    "### Eqs. (2) & (3) — Transformer encoder layer (pre-LN + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff8bea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoderLayer(nn.Module):\n",
    "    \"\"\"One transformer encoder block: LN→MSA→residual; LN→MLP→residual.\"\"\"\n",
    "    def __init__(self, dim, num_heads=12, mlp_ratio=4.0, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(dim)   # LN in Eq. (2)\n",
    "        self.msa = MultiHeadSelfAttention(dim, num_heads, proj_drop=drop)\n",
    "        self.ln2 = nn.LayerNorm(dim)   # LN in Eq. (3)\n",
    "        self.mlp = MLP(dim, mlp_ratio, drop=drop)\n",
    "\n",
    "    def forward(self, x):               # x: [B, N+1, D]\n",
    "        x = x + self.msa(self.ln1(x))   # Eq. (2): z'_ℓ\n",
    "        x = x + self.mlp(self.ln2(x))   # Eq. (3): z_ℓ\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc632362",
   "metadata": {},
   "source": [
    "### Full MiniViT — mapping Equations (1)–(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "907eb410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniViT(nn.Module):\n",
    "    \"\"\"Minimal ViT mapping directly to Equations (1)–(4).\"\"\"\n",
    "    def __init__(self, image_size=224, patch_size=16, in_chans=3,\n",
    "                 embed_dim=192, depth=2, num_heads=3, num_classes=10):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0\n",
    "        self.H = self.W = image_size // patch_size\n",
    "        self.num_patches = self.H * self.W\n",
    "\n",
    "        # Eq. (1): patch projection (x_p^i E), plus class token & positional embeddings\n",
    "        patch_dim = patch_size * patch_size * in_chans  # P^2 * C\n",
    "        self.proj = nn.Linear(patch_dim, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.num_patches, embed_dim))\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        # Eqs. (2)–(3): stack of encoder layers\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ViTEncoderLayer(embed_dim, num_heads=num_heads) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Eq. (4): final LN on class token, then a linear classifier\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # lightweight init\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "\n",
    "        # ---- Eq. (1): construct z0 ----\n",
    "        patches = self.unfold(x).transpose(1, 2)         # [B, N, P^2*C]\n",
    "        tokens = self.proj(patches)                      # [B, N, D]\n",
    "        cls = self.cls_token.expand(B, -1, -1)          # [B, 1, D]\n",
    "        z0 = torch.cat([cls, tokens], dim=1) + self.pos_embed\n",
    "        print(\"Eq.(1) z0:\", z0.shape)\n",
    "\n",
    "        # ---- Eqs. (2) & (3): transformer encoder ----\n",
    "        z = z0\n",
    "        for i, blk in enumerate(self.blocks, 1):\n",
    "            z = blk(z)\n",
    "            print(f\"After layer {i}: {z.shape}\")\n",
    "\n",
    "        # ---- Eq. (4): y = LN(z_L^0) ----\n",
    "        cls_final = z[:, 0]                # [B, D]\n",
    "        y = self.ln(cls_final)             # [B, D]\n",
    "        print(\"Eq.(4) y (pre-head):\", y.shape)\n",
    "        logits = self.head(y)              # [B, num_classes]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e37096",
   "metadata": {},
   "source": [
    "### Run a quick forward pass and print the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c37ba818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eq.(1) z0: torch.Size([1, 197, 192])\n",
      "After layer 1: torch.Size([1, 197, 192])\n",
      "After layer 2: torch.Size([1, 197, 192])\n",
      "Eq.(4) y (pre-head): torch.Size([1, 192])\n",
      "Logits: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Create dummy input and run the model\n",
    "x = torch.randn(1, 3, 224, 224) # (B, C, H, W)\n",
    "model = MiniViT(image_size=224, patch_size=16, embed_dim=192, depth=2, num_heads=3, num_classes=10)\n",
    "logits = model(x)\n",
    "print(\"Logits:\", logits.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
