<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> GenAI with LLMs (2) Pre-training &amp; Scaling laws | Wenwen Kong </title> <meta name="author" content="Wenwen Kong"> <meta name="description" content="Course summary of Generative AI with Large Language Models. Part Two."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wenwenkong.github.io/blog/2025/gen-ai-llm-2/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Wenwen</span> Kong </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Notes </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">GenAI with LLMs (2) Pre-training &amp; Scaling laws</h1> <p class="post-meta"> Created in June 16, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   ·   <a href="/blog/category/course-summary"> <i class="fa-solid fa-tag fa-sm"></i> course-summary</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This post covers LLM pre-training and scaling laws from the <strong>Generative AI With LLMs</strong> course offered by DeepLearning.AI.</p> <h2 id="model-cards">Model cards</h2> <p>Model cards are useful for understanding how a model is trained, its use case, and known limitations. See <a href="https://huggingface.co/docs/hub/en/model-cards" rel="external nofollow noopener" target="_blank">here</a> for a more complete definition from HuggingFace.</p> <p>Figures 1-2 are two example model cards from <a href="https://arxiv.org/abs/1810.03993" rel="external nofollow noopener" target="_blank">Mitchel et al. (2018)</a>. Figure 3 is the model card for T5 Large, captured from the lecture slides.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/1_modelcards_example-480.webp 480w,/assets/img/posts/genai_llm_2/1_modelcards_example-800.webp 800w,/assets/img/posts/genai_llm_2/1_modelcards_example-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/1_modelcards_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 1.</b> Model card example from Mitchell et al. 2018. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/2_modelcards_example-480.webp 480w,/assets/img/posts/genai_llm_2/2_modelcards_example-800.webp 800w,/assets/img/posts/genai_llm_2/2_modelcards_example-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/2_modelcards_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 2.</b> Model card example from Mitchell et al. 2018. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/3_modelcards_T5-480.webp 480w,/assets/img/posts/genai_llm_2/3_modelcards_T5-800.webp 800w,/assets/img/posts/genai_llm_2/3_modelcards_T5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/3_modelcards_T5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 3.</b> Model card for T5. Source: course lecture. </div> <h2 id="token-filtering">Token filtering</h2> <p>Only 1-3% of original tokens are used to train the LLM. We should consider this when we estimate how much data we need to collect if we decide to pre-train our own model. In other words, data quality control or bias removal is always needed to increase data quality before training. This refers to the <b>token filtering</b> or <b>rejection process</b> during LLM pre-training.</p> <p>LLM developers all start from massive web-scale corpora, then spend huge effort on <b>filtering</b>, <b>deduplicating</b>, <b>scoring for quality</b>, and <b>weighting data by domain or value</b>. See <a href="https://arxiv.org/abs/2303.08774" rel="external nofollow noopener" target="_blank">GPT-4 Technical Report</a>, <a href="https://arxiv.org/abs/2302.13971" rel="external nofollow noopener" target="_blank">LLaMA</a>, and <a href="https://arxiv.org/abs/2203.15556" rel="external nofollow noopener" target="_blank">Chinchilla</a> for details.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/4_token_filtering-480.webp 480w,/assets/img/posts/genai_llm_2/4_token_filtering-800.webp 800w,/assets/img/posts/genai_llm_2/4_token_filtering-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/4_token_filtering.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 4.</b> LLMs pretraining at high level. Source: course lecture. </div> <h2 id="llms-pre-training">LLMs pre-training</h2> <p>Different LLMs pretrain using different objectives - from denoising (such as BERT), to predicting next tokens (such as GPT), to reconstructing corrupted input using a sequence-to-sequence setup (such as T5 and BART). These design choices affect how models behave. For example, BERT is optimized for understanding tasks, GPT excels at generating fluent and coherent outputs. T5 and BART aim to balance both.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/5_llm_pretraining-480.webp 480w,/assets/img/posts/genai_llm_2/5_llm_pretraining-800.webp 800w,/assets/img/posts/genai_llm_2/5_llm_pretraining-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/5_llm_pretraining.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 5.</b> Model architectures and pre-training objectives. Source: course lecture. </div> <h3 id="autoencoding-models">Autoencoding models</h3> <p>Autoencoding models like BERT are pre-trained using <b>masked language modeling</b>. In the context of autoencoding models, the pre-training task is often described as a <b>“denoising” objective</b> because the model learns to reconstruct the original text from a corrupted (noisy) version of it.</p> <p>This setup provides the model with <b>bi-directional context</b>, meaning it can consider both preceding and following words when predicting masked tokens.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/6_autoencoding_pretraining-480.webp 480w,/assets/img/posts/genai_llm_2/6_autoencoding_pretraining-800.webp 800w,/assets/img/posts/genai_llm_2/6_autoencoding_pretraining-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/6_autoencoding_pretraining.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 6.</b> Autoencoding models pre-training. Source: course lecture. </div> <h3 id="autoregressive-models">Autoregressive models</h3> <p>Autoregressive models are pretrained using <b>causal language modeling</b>, where the model learns to predict the next token based solely on the preceding sequence of tokens. Note: predicting the next token is sometimes called <b>full language modeling</b> by researchers.</p> <p>This setup provides <b>unidirectional context</b>, meaning the model has no access to future tokens during training. Autoregressive models mask the input sequence and can only see the input tokens leading up to the token in question. The model has no knowledge of the end of the sequence. The model then iterates over the input sequence one by one to predict the following token.</p> <p>Large decoder-only models, such as GPT, demonstrate strong ability in zero-shot and few-shot inferences, and can generalize well across a wide range of language tasks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/7_autoregressive_pretraining-480.webp 480w,/assets/img/posts/genai_llm_2/7_autoregressive_pretraining-800.webp 800w,/assets/img/posts/genai_llm_2/7_autoregressive_pretraining-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/7_autoregressive_pretraining.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 7.</b> Autoregressive models pre-training. Source: course lecture. </div> <h3 id="seq2seq-models">Seq2Seq models</h3> <p>Exact detail of pre-training varies from model to model.</p> <ul> <li>A popular Seq2Seq model <a href="https://github.com/google-research/text-to-text-transfer-transformer" rel="external nofollow noopener" target="_blank">T5</a> pretrains the encoder using <b>span corruption</b>, which masks random sequences of input tokens. Those masked sequences are then replaced with a unique sentinel token. <b>Sentinel tokens</b> are special tokens added to the vocabulary, but do not correspond to any actual word from the input text. The decoder is then tasked with reconstructing the masked token sequences auto-regressively. The output is the sentinel token followed by the predicted tokens.</li> <li>BART combines the ideas of masked language modeling and causal language modeling within a seq2seq (encoder-decoder) framework. It corrupts text with various noise functions (e.g., setence shuffling, token masking), then uses a decoder to reconstruct the original text.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/8_seq2seq_pretraining-480.webp 480w,/assets/img/posts/genai_llm_2/8_seq2seq_pretraining-800.webp 800w,/assets/img/posts/genai_llm_2/8_seq2seq_pretraining-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/8_seq2seq_pretraining.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 8.</b> Seq2Seq models pre-training. Source: course lecture. </div> <h2 id="model-size">Model size</h2> <p>The growth of model size is powerd by</p> <ul> <li>introduction of transformer</li> <li>access to massive datasets</li> <li>more powerful compute resources</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/9_model_size-480.webp 480w,/assets/img/posts/genai_llm_2/9_model_size-800.webp 800w,/assets/img/posts/genai_llm_2/9_model_size-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/9_model_size.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 9.</b> Model size evolution with time. Source: <a href="https://informationisbeautiful.net/visualizations/the-rise-of-generative-ai-large-language-models-llms-like-chatgpt/" target="_blank" rel="external nofollow noopener"> Information is beautiful.</a> </div> <h2 id="compute-challenge">Compute challenge</h2> <p><b>CUDA</b>, short for <b>Compute Unified Device Architecture</b>, is a collection of libraries and tools developed for NVIDIA GPUs to boost performance on common deep-learning operations, including matrix multiplication, among many others. Deep-learning libraries such as PyTorch and TensorFlow use CUDA extensively to handle the low-level, hardware-specific details, including data movement between CPU and GPU memory.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/10_cuda-480.webp 480w,/assets/img/posts/genai_llm_2/10_cuda-800.webp 800w,/assets/img/posts/genai_llm_2/10_cuda-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/10_cuda.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 10.</b> CUDA out-of-memory error (<a href="https://www.oreilly.com/library/view/generative-ai-on/9781098159214/ch04.html" target="_blank" rel="external nofollow noopener">source</a>). </div> <p>Quote from <em>Generative AI on AWS: Building Context-Aware Multimodal Reasoning Applications (<a href="https://www.oreilly.com/library/view/generative-ai-on/9781098159214/ch04.html" rel="external nofollow noopener" target="_blank">source</a>)</em>:</p> <blockquote> <p>A single-model parameter, at full 32-bit precision, is represented by 4 bytes. Therefore, a 1-billion-parameter model requires 4GB of GPU RAM just to load the model into GPU RAM at full precision. If you want to also train the model, you need more GPU memory to store the states of the numerical optimizer, gradients, and activations, as well as any temporary variables used by your functions.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/11_gpuram_store_1b-480.webp 480w,/assets/img/posts/genai_llm_2/11_gpuram_store_1b-800.webp 800w,/assets/img/posts/genai_llm_2/11_gpuram_store_1b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/11_gpuram_store_1b.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 11.</b> Approximate GPU RAM needed to store 1B parameters. Source: course lecture. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/12_gpuram_train_1b-480.webp 480w,/assets/img/posts/genai_llm_2/12_gpuram_train_1b-800.webp 800w,/assets/img/posts/genai_llm_2/12_gpuram_train_1b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/12_gpuram_train_1b.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 12.</b> Additional GPU RAM needed to train 1B parameters. Source: course lecture </div> <p>Quote from <em>Generative AI on AWS: Building Context-Aware Multimodal Reasoning Applications (<a href="https://www.oreilly.com/library/view/generative-ai-on/9781098159214/ch04.html" rel="external nofollow noopener" target="_blank">source</a>)</em>:</p> <blockquote> <p>When you experiment with training a model, it’s recommended that you start with batch_size=1 to find the memory boundaries of the model with just a single training example. You can then incrementally increase the batch size until you hit the CUDA out-of-memory error. This will determine the maximum batch size for the model and dataset. A larger batch size can often speed up your model training.</p> </blockquote> <blockquote> <p>These additional components lead to approximately 12–20 extra bytes of GPU memory per model parameter. For example, to train a 1-billion-parameter model, you will need approximately 24 GB of GPU RAM at 32-bit full precision, six times the memory compared to just 4 GB of GPU RAM for loading the model</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/13_gpuram_load_vs_loadtrain-480.webp 480w,/assets/img/posts/genai_llm_2/13_gpuram_load_vs_loadtrain-800.webp 800w,/assets/img/posts/genai_llm_2/13_gpuram_load_vs_loadtrain-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/13_gpuram_load_vs_loadtrain.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 13.</b> Comparison of approximate GPU RAM needed to load versus load and train a 1-billion-parameter model at 32-bit full precision (<a href="https://www.oreilly.com/library/view/generative-ai-on/9781098159214/ch04.html" target="_blank" rel="external nofollow noopener">source</a>). </div> <p>One technique we can use to reduce memory is called <b>quantization</b>. The idea is to reduce the memory required to store the model weights by reducing their precision from 32-bit floating point numbers to 16-bit (or 8-bit) floating point numbers. Quantization statistically projects the original 32-bit floating point numbers into a lower precision space, using scaling factors calculated based on the range of the original 32-bit floating point numbers.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/14_FP16-480.webp 480w,/assets/img/posts/genai_llm_2/14_FP16-800.webp 800w,/assets/img/posts/genai_llm_2/14_FP16-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/14_FP16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 14.</b> Quantization: FP16. Source: course lecture </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/15_BFLOAT16-480.webp 480w,/assets/img/posts/genai_llm_2/15_BFLOAT16-800.webp 800w,/assets/img/posts/genai_llm_2/15_BFLOAT16-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/15_BFLOAT16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 15.</b> Quantization: BFLOAT16. Source: course lecture </div> <p><b>BFLOAT-16</b> (BF16, developed at Google Brain) has recently become a popular alternative to FP16.</p> <ul> <li>BF16 is a hybrid between half precision FP16 and full precision FP32.</li> <li>BF16 significantly helps with training stability and is supported by newer GPUs such as NVIDIA’s A100.</li> <li>BF16 is often described as truncated 32-bit float, as it captures the full dynamic range of the full 32-bit float that uses only 16-bits.</li> <li>BF16 uses the full 8 bits to represent the exponent, but truncates the fraction to just 7 bits.</li> <li>Downside: BF16 is not well suited for integer calculations, but these are relatively rare in deep learning.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/16_INT8-480.webp 480w,/assets/img/posts/genai_llm_2/16_INT8-800.webp 800w,/assets/img/posts/genai_llm_2/16_INT8-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/16_INT8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 16.</b> Quantization: INT8. Source: course lecture </div> <h2 id="scalling-law">Scalling law</h2> <p>The <b>Chinchilla scaling law</b> shows that, for a fixed compute budget, language model performance is optimized by using more training data and a smaller model size (<a href="https://arxiv.org/abs/2203.15556" rel="external nofollow noopener" target="_blank">Hoffmann et al., 2022</a>). Undertraining is the bottleneck - large models like GPT-3 were not trained on enough tokens to fully benefit from their size.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/17_optimal_computing-480.webp 480w,/assets/img/posts/genai_llm_2/17_optimal_computing-800.webp 800w,/assets/img/posts/genai_llm_2/17_optimal_computing-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/17_optimal_computing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 17.</b> Compute optimal models. Source: course lecture </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/18_scalinglaw-480.webp 480w,/assets/img/posts/genai_llm_2/18_scalinglaw-800.webp 800w,/assets/img/posts/genai_llm_2/18_scalinglaw-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/18_scalinglaw.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 18.</b> Chinchilla scaling laws for model and dataset size. Source: course lecture </div> <h2 id="domain-adaptation">Domain adaptation</h2> <p>Example: <a href="https://arxiv.org/abs/2303.17564" rel="external nofollow noopener" target="_blank">BloombergGPT</a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/genai_llm_2/19_BloombergGPT-480.webp 480w,/assets/img/posts/genai_llm_2/19_BloombergGPT-800.webp 800w,/assets/img/posts/genai_llm_2/19_BloombergGPT-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/genai_llm_2/19_BloombergGPT.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 19.</b> BloombergGPT: domain adaptation for finance. Source: course lecture </div> <h2 id="references">References</h2> <ul> <li> <a href="https://www.amazon.com/Generative-AI-AWS-Multimodal-Applications/dp/1098159225/" rel="external nofollow noopener" target="_blank">Generative AI on AWS: Building Context-Aware, Multimodal Reasoning Applications</a> <ul> <li>Deep dive into all phases of the generative AI Lifecycle</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2211.05100" rel="external nofollow noopener" target="_blank">BLOOM: BigScience 176B Model</a> <ul> <li>BLOOM is an open-source LLM with 176B parameters trained in an open and transparent way. In this paper, the authors present a detailed discussion of the dataset and process used to train the model. You can also see a high-level overview of the model <a href="https://bigscience.notion.site/BLOOM-BigScience-176B-Model-ad073ca07cdf479398d5f95d88e218c4" rel="external nofollow noopener" target="_blank">here</a> </li> </ul> </li> <li> <a href="https://www.coursera.org/learn/classification-vector-spaces-in-nlp/home/week/3" rel="external nofollow noopener" target="_blank">Vector Space Models</a> <ul> <li>Series of lessons from DeepLearning.AI’s Natural Language Processing specialization discussing the basics of vector space models and their use in language modeling.</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2001.08361" rel="external nofollow noopener" target="_blank">Scaling Laws for Neural Language Models</a> <ul> <li>Pre-training and scaling laws</li> </ul> </li> <li> <a href="https://arxiv.org/pdf/2204.05832.pdf" rel="external nofollow noopener" target="_blank">What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?</a> Model architectures and pre-training objectives. The paper examines modeling choices in large pre-trained language models and identifies the optimal approach for zero-shot generalization.</li> <li> <a href="https://huggingface.co/tasks" rel="external nofollow noopener" target="_blank">HuggingFace Tasks</a> and <a href="https://huggingface.co/models" rel="external nofollow noopener" target="_blank">Model Hub</a> <ul> <li>Collection of resources to tackle varying machine learning tasks using the HuggingFace library.</li> </ul> </li> <li> <a href="https://arxiv.org/pdf/2302.13971.pdf" rel="external nofollow noopener" target="_blank">LLaMA: Open and Efficient Foundation Language Models</a> <ul> <li>Article from Meta AI proposing Efficient LLMs (their model with 13B parameters outperform GPT3 with 175B parameters on most benchmarks)</li> </ul> </li> <li> <a href="https://arxiv.org/pdf/2005.14165.pdf" rel="external nofollow noopener" target="_blank">Language Models are Few-Shot Learners</a> <ul> <li>Scaling laws and compute-optimal models. This paper investigates the potential of few-shot learning in Large Language Models.</li> </ul> </li> <li> <a href="https://arxiv.org/pdf/2203.15556.pdf" rel="external nofollow noopener" target="_blank">Training Compute-Optimal Large Language Models</a> <ul> <li>Study from DeepMind to evaluate the optimal model size and number of tokens for training LLMs. Also known as “Chinchilla Paper”.</li> </ul> </li> <li> <a href="https://arxiv.org/pdf/2303.17564.pdf" rel="external nofollow noopener" target="_blank">BloombergGPT: A Large Language Model for Finance</a> <ul> <li>LLM trained specifically for the finance domain, a good example that tried to follow chinchilla laws.</li> </ul> </li> </ul> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Wenwen Kong. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 17, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-research",title:"Research",description:"",section:"Navigation",handler:()=>{window.location.href="/research/"}},{id:"nav-notes",title:"Notes",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-teaching",title:"Teaching",description:"Courses that I've taught and students that I've mentored.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"post-genai-with-llms-2-pre-training-amp-scaling-laws",title:"GenAI with LLMs (2) Pre-training & Scaling laws",description:"Course summary of Generative AI with Large Language Models. Part Two.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gen-ai-llm-2/"}},{id:"post-genai-with-llms-1-fundamentals",title:"GenAI with LLMs (1) Fundamentals",description:"Course summary of Generative AI with Large Language Models. Part One.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gen-ai-llm-1/"}},{id:"post-predicting-u-s-soybean-yield-with-climate-data",title:"Predicting U.S. Soybean Yield with Climate Data",description:"An end-to-end machine learning project on U.S. soybean yield prediction",section:"Posts",handler:()=>{window.location.href="/blog/2025/us-soybean-prediction/"}},{id:"post-statistical-learning-notes-outline",title:"Statistical Learning Notes Outline",description:"Outline of notes about statistical learning.",section:"Posts",handler:()=>{window.location.href="/blog/2023/sl-notes-outline/"}},{id:"post-statistics-and-climate-journal-club",title:"Statistics and Climate Journal Club",description:"Archived topics discussed in the Statistics and Climate Journal Club at UCLA.",section:"Posts",handler:()=>{window.location.href="/blog/2022/stats-climate-journalclub/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=UDAPrT8AAAAJ","_blank")}},{id:"socials-publons",title:"Publons",section:"Socials",handler:()=>{window.open("https://publons.com/a/Y-8516-2019/","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/wenwenkong","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/wenwenkong","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>