<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Vision Transformer (ViT) Notes | Wenwen Kong </title> <meta name="author" content="Wenwen Kong"> <meta name="description" content="Learning notes and mini-implementation of ViT"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wenwenkong.github.io/blog/2025/ViT-notes/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Wenwen</span> Kong </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Notes </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Vision Transformer (ViT) Notes</h1> <p class="post-meta"> Created on November 08, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer-vision</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This post focuses on understanding the core idea and model architecture of the Vision Transformer (ViT).</p> <p>Before ViT, convolutional architectures remained dominant in computer vision. Classic Convolutional Neural Networks (CNNs) such as VGG and especially ResNet, with its residual skip connections, were state of the art. In the original ViT paper (<a href="https://arxiv.org/abs/2010.11929" rel="external nofollow noopener" target="_blank">Dosovitskiy et al., 2021, An image is worth 16 x 16 words: transformers for image recognition at scale</a>), the authors applied transformers to image classification tasks and found that, when trained on large datasets, ViT can match or even surpass the performance of top convolutional networks.</p> <hr> <h2 id="core-idea">Core idea</h2> <h4 id="from-words-to-patches">From words to patches</h4> <p>ViT processes sequences of image patches just as transformers process sequences of word tokens in natural language processing (NLP). In NLP, a transformer takes a sequence of word embeddings as input and learns contextual relationships among them through self-attention. Similarly, ViT splits an image into fixed-size patches, flattens each patch, linearly embeds the patches into vectors, and feeds the resulting sequence of patch embeddings into a standard transformer encoder.</p> <p>Besides this <em>words vs patches</em> analogy, ViT also mirrors how transformers are trained in NLP. Dosovitskiy et al. (2021) note that:</p> <blockquote> <p>The dominant approach [in NLP] is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset.</p> </blockquote> <blockquote> <p>Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task.</p> </blockquote> <p>ViT follows a similar pattern: it excels when first trained on large image datasets and then fine-tuned for smaller, domain-specific tasks. We will get to this point later in the post when discussing why ViT benefits so much from large-scale pretraining. For a deeper look at pretraining in large language models, see my <a href="https://wenwenkong.com/blog/2025/gen-ai-llm-2/" rel="external nofollow noopener" target="_blank">previous post</a>.</p> <h4 id="cnns-vs-vit">CNNs vs. ViT</h4> <p>In machine learning, <em>inductive bias</em> refers to the built-in assumptions a model makes about data before seeing any examples. Dosovitskiy et al. (2021) noted that ViT has much less image-specific inductive bias than CNNs, quote:</p> <blockquote> <p>… In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global.</p> </blockquote> <p>The table below summarizes how CNNs and ViT differ in their built-in assumptions about images.</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f4f4f4;"> <th style="border: 1px solid #ddd; padding: 8px;">Inductive biases</th> <th style="border: 1px solid #ddd; padding: 8px;">CNNs</th> <th style="border: 1px solid #ddd; padding: 8px;">ViTs</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;"><b>Locality</b></td> <td style="border: 1px solid #ddd; padding: 8px;">Convolution filters look at local pixel neighborhoods (small receptive fields).</td> <td style="border: 1px solid #ddd; padding: 8px;">No built-in locality; all patches can attend to each other globally.</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="border: 1px solid #ddd; padding: 8px;"><b>2D neighborhood</b></td> <td style="border: 1px solid #ddd; padding: 8px;">Operates directly on 2D image grids; spatial adjacency is hard-coded.</td> <td style="border: 1px solid #ddd; padding: 8px;">Flattened into 1D sequence of patches; 2D structure not explicit.</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;"><b>Translation equivariance</b></td> <td style="border: 1px solid #ddd; padding: 8px;">A shifted input produces shifted feature maps (invariance to object position).</td> <td style="border: 1px solid #ddd; padding: 8px;">No inherent equivariance; positional embeddings must be learned.</td> </tr> </table> <h4 id="why-vit-excels-with-large-datasets">Why ViT excels with large datasets</h4> <p>Because ViT does not come with strong image-specific inductive biases, it treats all patches equally and relies on self-attention to decide which regions of an image are important. This explains why ViT performs worse on small datasets but excels on large ones.</p> <p>When the training dataset is small, CNNs often outperform ViTs because their built-in assumptions help them learn meaningful patterns even with limited data. In contrast, ViT has to learn locality, edges, textures, and other visual structures from scratch, making it more data-hungry and less efficient at small scales. With large datasets, however, ViT can learn spatial and structural patterns directly from data rather than relying on hard-coded assumptions. Its global self-attention becomes a major advantage, enabling the model to capture long-range dependencies across the entire image and ultimately surpass CNNs.</p> <hr> <h2 id="architecture">Architecture</h2> <p>In this section, we walk through the ViT model architecture using Figure 1 and Equations (1)-(4) from Dosovitskiy et al. (2021). Figure 1 (below) is an annotated version of the original figure from the paper, showing how an image is divided into patches, embedded, and processed through the transformer encoder. Google Research <a href="https://research.google/blog/transformers-for-image-recognition-at-scale/" rel="external nofollow noopener" target="_blank">blog</a> also provides an animated visualization that illustrates the ViT workflow.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/vit_notes/1_vit_architecture_annotated-480.webp 480w,/assets/img/posts/vit_notes/1_vit_architecture_annotated-800.webp 800w,/assets/img/posts/vit_notes/1_vit_architecture_annotated-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/vit_notes/1_vit_architecture_annotated.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 1.</b> Annotated ViT architecture (adapted from Dosovitskiy et al., 2021). </div> <p>Here is how ViT is defined mathematically:</p> \[\begin{aligned} \mathbf{z}_0 &amp;= \big[\, x_{\text{class}};\; x_{p}^{1}\,\mathbf{E};\; x_{p}^{2}\,\mathbf{E};\; \dots;\; x_{p}^{N}\,\mathbf{E} \,\big] + E_{\text{pos}}, \quad \mathbf{E}\in \mathbb{R}^{(P^{2}\!\cdot C)\times D},\; E_{\text{pos}}\in \mathbb{R}^{(N+1)\times D} \quad &amp;(1) \\ \mathbf{z}'_{\ell} &amp;= \mathrm{MSA}\!\left(\mathrm{LN}(\mathbf{z}_{\ell-1})\right) + \mathbf{z}_{\ell-1}, \quad \ell=1,\dots,L \quad &amp;(2) \\ \mathbf{z}_{\ell} &amp;= \mathrm{MLP}\!\left(\mathrm{LN}(\mathbf{z}'_{\ell})\right) + \mathbf{z}'_{\ell}, \quad \ell=1,\dots,L \quad &amp;(3) \\ \mathbf{y} &amp;= \mathrm{LN}\!\left(\mathbf{z}^{\,0}_{L}\right) \quad &amp;(4) \end{aligned}\] <h4 id="equation-1-embedding">Equation (1): embedding</h4> <p>Equation (1) corresponds to the embedding stage in Figure 1. The input image is represented as \(x \in \mathbb{R}^{H \times W \times C}\), where \(H\), \(W\), and \(C\) denote height, width, and number of channels. The image is divided into \(N\) patches, \(x_p \in \mathbb{R}^{N \times (P^2 \cdot C)}\), where \((P, P)\) represents the resolution of each image patch. Each patch vector is multiplied by a learnable projection matrix \(E\) to get a \(D\)-dimensional embedding. A special classification token \(x{\text{class}}\) is prepended - its embedding will later represent the whole image. A positional embedding \(E_{\text{pos}}\) is also added to retain spatial information. These form the input sequence \(\mathbf{z}_0\) that will be feeded to the transformer.</p> <p><strong>Understanding constant \(D\)</strong></p> <p>Transformers are designed to process sequences where every token has the same dimensionality across all layers. This uniformity makes stacking layers simple: each layer takes a sequence of vectors of shape \([N \times D]\) and outputs another sequence \([N \times D]\). Here, \(D\) indicates the <strong>embedding dimension</strong> (sometimes called the <em>hidden size</em>). Every patch embedding and every intermediate vector inside the transformer has size \(D\).</p> <p>\(D\) is a hyperparameter. With enough data, a larger \(D\) usually means higher model capacity and better performance — but it also requires more compute and memory. In a nutshell, \(D\) is a design choice controlling how much information each token can represent and how big the transformer is.</p> <p><strong>Understanding the trainable linear layer \(\mathbf{E}\)</strong></p> <p>A raw image patch of size \(P \times P \times C\) is flattened into a vector, but the transformer expects tokens of dimension \(D\). So a trainable linear layer (denoted as \(\mathbf{E}\)) maps flattened patches into the \(D\)-dimensional embedding space. After this step, all tokens (patches + [CLS]) live in the same latent space.</p> <p>Let’s walk through the logic using the ViT-Base configuration. Suppose the image size is \(224 \times 224 \times 3\), patch size is \(16 \times 16\), and embedding dimension \(D = 768\).</p> <ol> <li> <p><strong>Split into patches.</strong><br> Each patch = \(16 \times 16 \times 3 = 768\) raw pixel values.<br> Total number of patches = \((224 / 16)^2 = 196\).</p> </li> <li> <p><strong>Flatten each patch.</strong><br> Flatten \(16 \times 16 \times 3\) into a vector of length 768.<br> Before projection, each patch is a 768-dimensional vector of raw pixel intensities.</p> </li> <li> <p><strong>Project to the embedding dimension \(D\).</strong><br> Note: although the patch length (768) and \(D\) (768) are identical in this example (ViT-Base), they are <em>not the same thing</em>.<br> The flattened patch values are <strong>raw pixel values</strong>, while \(D\) represents the <strong>dimension of learned embeddings</strong> that the transformer requires in a consistent latent space.<br> Even when \(P \times P \times C = D\), we still need a linear projection — it’s not about matching dimensions, it’s about <em>learning a mapping from raw pixels → latent embedding space</em>.</p> <p>ViT applies a trainable linear layer \(\mathbf{E}\):</p> \[\mathbf{e} = \mathbf{W} \mathbf{x} + \mathbf{b}\] <p>where:</p> <ul> <li>\(\mathbf{x}\) = flattened patch (size \(P^2 C\), here 768)</li> <li>\(\mathbf{W}\) = learnable weight matrix (size \((P^2 C) \times D\))</li> <li>\(\mathbf{b}\) = bias term (size \(D\))</li> <li>\(\mathbf{e}\) = patch embedding (size \(D\))</li> </ul> </li> <li> <p><strong>Build the sequence.</strong><br> Each of the 196 patches is now projected to a 768-dimensional embedding vector.<br> Add one learnable [CLS] token (also 768-dimensional). The sequence length becomes 197, each of size 768 — so the input to the transformer is a \([197 \times 768]\) matrix.</p> </li> <li> <p><strong>Transformer layers.</strong><br> Each transformer layer takes a \([197 \times 768]\) input and outputs another \([197 \times 768]\) matrix.</p> </li> </ol> <h4 id="equation-2-multi-head-self-attention-msa">Equation (2): multi-head self-attention (MSA)</h4> <p>Equation (2) represents the self-attention sub-layer in each transformer block. Inside each transformer block, the input passes through <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html" rel="external nofollow noopener" target="_blank">layer normalization (LN)</a> and then <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html" rel="external nofollow noopener" target="_blank">multi-head self-attention (MSA)</a>: \(\mathrm{MSA}\!\left(\mathrm{LN}(\mathbf{z}_{\ell-1})\right)\). Each token (patch embedding) attends to all others globally, capturing relationships across the entire image. The residual connection \(\mathbf{z}_{\ell-1}\) (skip arrow in Figure 1) helps stabilize gradient flow during training.</p> <h4 id="equation-3-feed-forward-mlp">Equation (3): feed-forward (MLP)</h4> <p>Equation (3) describes the feed-forward network (or <a href="https://scikit-learn.org/stable/modules/neural_networks_supervised.html" rel="external nofollow noopener" target="_blank">MLP</a>) that follows self-attention. After the attention step, the output is normalized again and passed through a two-layer MLP (with a <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.GELU.html" rel="external nofollow noopener" target="_blank">GELU</a> activation in between): \(\mathrm{MLP}(x) = W_2 \, \mathrm{GELU}(W_1 x + b_1) + b_2\). Another residual connection adds the input back to the output.</p> <p>Together, Equations (2) and (3) make up a transformer block - stacked \(L\) times as shown in Figure 1.</p> <h4 id="equation-4-classification-head">Equation (4): classification head</h4> <p>Equation (4) corresponds to the output head of the model. After L transformer blocks, the output (i.e., the class token embedding \(\mathbf{z}^{0}_{L}\) that represents the entire image) is extracted. \(\mathbf{z}^{0}_{L}\) is layer-normalized and used for classification through a linear classifier. The linear classifier in ViT can be viewed as a standard logistic regression layer that takes the final class token embedding as input and outputs logits for each class.</p> <hr> <h2 id="implementation">Implementation</h2> <p>In this section, we demonstrate a jupyter notebook that maps the core ViT equations to a minimal PyTorch implementation. It follows the paper’s equations (1) to (4) and mirrors the structure of open-source ViT implementations (<a href="https://github.com/google-research/vision_transformer" rel="external nofollow noopener" target="_blank">Google Research</a>, <a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py" rel="external nofollow noopener" target="_blank">timm</a>, <a href="https://huggingface.co/docs/transformers/model_doc/vit" rel="external nofollow noopener" target="_blank">Hugging Face</a>).</p> <details class="notebook-frame"> <summary class="notebook-header"> ▶️ Notebook: ViT — From Equations to Code (click to expand) </summary> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/posts/vit_notes/vit_mini_implementation.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> </details> <hr> <h2 id="references">References</h2> <ul> <li> <p><strong>Dosovitskiy, A.</strong>, Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., &amp; Houlsby, N. (2021). <em>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.</em> <a href="https://arxiv.org/abs/2010.11929" rel="external nofollow noopener" target="_blank">Paper</a></p> </li> <li> <p><strong>Google Research ViT</strong>: <a href="https://github.com/google-research/vision_transformer" rel="external nofollow noopener" target="_blank">Repo</a></p> </li> <li> <p><strong>timm: PyTorch Image Models</strong> (Ross Wightman) Vision Transformer source: <a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py" rel="external nofollow noopener" target="_blank">Code</a></p> </li> <li> <p><strong>Hugging Face Transformers — Vision Transformer (ViT)</strong>: <a href="https://huggingface.co/docs/transformers/model_doc/vit" rel="external nofollow noopener" target="_blank">Documentation</a>, <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/modeling_vit.py" rel="external nofollow noopener" target="_blank">code</a></p> </li> </ul> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Wenwen Kong. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: November 09, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-research",title:"Research",description:"",section:"Navigation",handler:()=>{window.location.href="/research/"}},{id:"nav-notes",title:"Notes",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-teaching",title:"Teaching",description:"Courses that I've taught and students that I've mentored.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"post-vision-transformer-vit-notes",title:"Vision Transformer (ViT) Notes",description:"Learning notes and mini-implementation of ViT",section:"Posts",handler:()=>{window.location.href="/blog/2025/ViT-notes/"}},{id:"post-genai-with-llms-6-llm-powered-applications",title:"GenAI with LLMs (6) LLM powered applications",description:"Course summary of Generative AI with Large Language Models. Part Six.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gen-ai-llm-6/"}},{id:"post-genai-with-llms-5-reinforcement-learning-from-human-feedback",title:"GenAI with LLMs (5) Reinforcement learning from human feedback",description:"Course summary of Generative AI with Large Language Models. Part Five.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gen-ai-llm-5/"}},{id:"post-genai-with-llms-4-parameter-efficient-fine-tuning",title:"GenAI with LLMs (4) Parameter-efficient fine-tuning",description:"Course summary of Generative AI with Large Language Models. Part Four.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gen-ai-llm-4/"}},{id:"post-genai-with-llms-3-instruction-fine-tuning",title:"GenAI with LLMs (3) Instruction fine-tuning",description:"Course summary of Generative AI with Large Language Models. Part Three.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gen-ai-llm-3/"}},{id:"post-genai-with-llms-2-pre-training",title:"GenAI with LLMs (2) Pre-training",description:"Course summary of Generative AI with Large Language Models. Part Two.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gen-ai-llm-2/"}},{id:"post-genai-with-llms-1-fundamentals",title:"GenAI with LLMs (1) Fundamentals",description:"Course summary of Generative AI with Large Language Models. Part One.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gen-ai-llm-1/"}},{id:"post-predicting-u-s-soybean-yield-with-climate-data",title:"Predicting U.S. Soybean Yield with Climate Data",description:"An end-to-end machine learning project on U.S. soybean yield prediction",section:"Posts",handler:()=>{window.location.href="/blog/2025/us-soybean-prediction/"}},{id:"post-statistical-learning-notes-outline",title:"Statistical Learning Notes Outline",description:"Outline of notes about statistical learning.",section:"Posts",handler:()=>{window.location.href="/blog/2023/sl-notes-outline/"}},{id:"post-statistics-and-climate-journal-club",title:"Statistics and Climate Journal Club",description:"Archived topics discussed in the Statistics and Climate Journal Club at UCLA.",section:"Posts",handler:()=>{window.location.href="/blog/2022/stats-climate-journalclub/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=UDAPrT8AAAAJ","_blank")}},{id:"socials-publons",title:"Publons",section:"Socials",handler:()=>{window.open("https://publons.com/a/Y-8516-2019/","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/wenwenkong","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/wenwenkong","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>